{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETaj7dxJ3HQ8"
   },
   "source": [
    "# AI stock analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PauliusU/AI-stock-analysis/blob/master/AI-stock-analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary, goals and methodology"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Project aims to use compare 3 different approaches to predict stock prices and choose the best one.\n",
    "\n",
    "Project uses combinations of models based on neural networks (`LSTM` and `GRU`) and a linear model (`ARIMA`). This is due to a fact that time series data often contain both linear and nonlinear patterns. Therefore, neither ARIMA nor neural networks can be adequate in modeling and predicting time series data. The ARIMA model cannot deal with nonlinear relationships while the neural network model alone is not able to handle both linear and nonlinear patterns equally well.\n",
    "\n",
    "All models are trained with the same data set - price of Tesla shares for the last 5 years:\n",
    "1. First approach is price prediction using `ARIMA` (Auto Regressive Integrated Moving Average) model. ❗ N.B. ARIMA was not used during our AI course ❗\n",
    "2. Another approach uses `LSTM` (Long Short-Term memory) neural network — probably the most popular machine learning approach for stock market prediction.\n",
    "3. Final approach utilizes `GRU` (Gated Recurrent Unit) network.\n",
    "\n",
    "Final comparison of each approach is measured by the `RMSE` (root-mean-square error). The lower the error, the better the model is.\n",
    "\n",
    "The comparison result shows that in our experiments the GRU method was the most accurate, followed by the LSTM method in the second place and ARIMA in the third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This section uses Yahoo Finance API to fetch share price data for the last 5 years and visualizes result in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Yahoo Finance's API wrapper\n",
    "!pip install yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "print(f\"yfinance version: {yf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get stock data for the last 5 years\n",
    "\n",
    "ticker = 'TSLA'\n",
    "stock_data = yf.download(ticker, start='2017-11-01', end='2022-11-01')\n",
    "print(stock_data.head())\n",
    "print(stock_data.tail())\n",
    "\n",
    "# Variables used later in models\n",
    "close_prices = stock_data['Close']\n",
    "values = close_prices.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot closing price over time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title('Historical stock prices')\n",
    "plt.plot(close_prices)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Closing price in USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model#1 ARIMA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by fitting a simple ARIMA model to forecast the next value of the stock price.\n",
    "\n",
    "ARIMA, short for ‘Auto Regressive Integrated Moving Average’ is actually a class of models that \"explains\" a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values. Any ‘non-seasonal’ time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models.\n",
    "\n",
    "Here we use a grid search procedure to test several configurations of hyperparameters for the ARIMA model. Finally, only the best configuration (with smallest RMSE) is used for comparison with LSTM and GRU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search ARIMA parameters for time series\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Evaluate an ARIMA model for a given order (p,d,q)\n",
    "# An ARIMA model is characterized by 3 terms: p, d, q where:\n",
    "#       p is the order of the AR term\n",
    "#       q is the order of the MA term\n",
    "#       d is the number of differencing required to make the time series stationary\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "    # prepare training dataset\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    return rmse, predictions, history\n",
    "\n",
    "\n",
    "# Evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg, best_predictions, best_history, = float(\n",
    "        \"inf\"), None, list(), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p, d, q)\n",
    "                try:\n",
    "                    rmse, predictions, history = evaluate_arima_model(dataset,\n",
    "                                                                      order)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg, best_predictions, best_history = rmse, order, predictions, history\n",
    "                    print(f\"ARIMA{order} RMSE={rmse:.3f}\")\n",
    "                except:\n",
    "                    continue\n",
    "    print(f\"Best ARIMA{best_cfg} RMSE={best_score:.3f}\")\n",
    "    return best_score, best_predictions, best_history\n",
    "\n",
    "\n",
    "# Evaluate parameters. Increasing ranges takes more time, but can result in more accurate model. For example, p_values = [0, 1, 2, 4, 6, 8, 10]\n",
    "p_values = [0]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rmse_arima, predictions_arima, history_arima = evaluate_models(values, p_values,\n",
    "                                                               d_values,\n",
    "                                                               q_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare training and test sets for RNNs (LSTM and GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To prepare data to train and test models, only the closing price would be needed. Cell below gets 80% of these data points for closing price, normalizes them, and converts them into 2D array.\n",
    "\n",
    "As we use LSTM and GRU neural networks we also need to scale the data in the column ‘Close’ because the machine learning algorithm works much better with scaled than with regular data. Closing price is scaled in the range between 0 and 1. This is just the preliminary operation which we need to execute to let our models work with better efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training set\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "training_data_len = int(len(values) * 0.8)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(values.reshape(-1, 1))\n",
    "train_data = scaled_data[0: training_data_len, :]\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i - 60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare test set\n",
    "\n",
    "test_data = scaled_data[training_data_len - 60:, :]\n",
    "x_test = []\n",
    "y_test = values[training_data_len:]\n",
    "\n",
    "for i in range(60, len(test_data)):\n",
    "    x_test.append(test_data[i - 60:i, 0])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model#2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if GPU is used\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\n",
    "    f\"Number of GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Print the list of available training devices - alternative method to verify TensorFlow sees the GPU\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up LSTM network architecture\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "model_lstm = keras.Sequential()\n",
    "# 100 neurons on the first layer of LSTM\n",
    "model_lstm.add(\n",
    "    keras.layers.LSTM(100, return_sequences=True,\n",
    "                      input_shape=(x_train.shape[1], 1)))\n",
    "# 100 neurons on the second layer of LSTM\n",
    "model_lstm.add(keras.layers.LSTM(100, return_sequences=False))\n",
    "# Dense with 25 neurons\n",
    "model_lstm.add(keras.layers.Dense(25))\n",
    "# Just one neuron which is our result on the last step of the model\n",
    "model_lstm.add(keras.layers.Dense(1))\n",
    "model_lstm.summary()\n",
    "\n",
    "# Set optimizer and loss function\n",
    "# Use optimizer ‘adam’ which is the most popular in tasks of stock price prediction\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the LSTM model\n",
    "\n",
    "# Teach model during 80 epochs (computations) with batch size on each epoch equal to 10 (1/10 of samples)\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Show progress bar with verbose=1\n",
    "history_lstm = model_lstm.fit(x_train, y_train, batch_size=BATCH_SIZE,\n",
    "                              epochs=EPOCHS,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluation of LSTM\n",
    "\n",
    "predictions_lstm = model_lstm.predict(x_test)\n",
    "predictions_lstm = scaler.inverse_transform(predictions_lstm)\n",
    "rmse_lstm = np.sqrt(np.mean(predictions_lstm - y_test) ** 2)\n",
    "print(rmse_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_learning_curves(loss, title):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.title(title)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "plot_learning_curves(history_lstm.history[\"loss\"], 'LSTM')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model#3 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up GRU network architecture\n",
    "\n",
    "model_gru = keras.Sequential()\n",
    "model_gru.add(keras.layers.GRU(units=50, return_sequences=True,\n",
    "                               input_shape=(x_train.shape[1], 1)))\n",
    "model_gru.add(keras.layers.GRU(units=50))\n",
    "model_gru.add(keras.layers.Dense(1))\n",
    "\n",
    "# Use the same optimizer and loss function as with LSTM\n",
    "# Use optimizer ‘adam’ which is the most popular in tasks of stock price prediction\n",
    "model_gru.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the GRU model\n",
    "\n",
    "# Teach model during 80 epochs (computations) with batch size on each epoch equal to 10 (1/10 of samples)\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Use the same EPOCHS and BATCH_SIZE params as in LSTM model. Show progress bar with verbose=1,\n",
    "history_gru = model_gru.fit(x_train, y_train, batch_size=BATCH_SIZE,\n",
    "                            epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluation of GRU model\n",
    "\n",
    "predictions_gru = model_gru.predict(x_test)\n",
    "predictions_gru = scaler.inverse_transform(predictions_gru)\n",
    "rmse_gru = np.sqrt(np.mean(predictions_gru - y_test) ** 2)\n",
    "print(rmse_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_learning_curves(history_gru.history[\"loss\"], 'GRU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Final results and conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(np.arange(len(history_lstm.history[\"loss\"])) + 0.5,\n",
    "         history_lstm.history[\"loss\"], \"b.-\", label=\"Training loss (LSTM)\")\n",
    "plt.plot(np.arange(len(history_gru.history[\"loss\"])) + 0.5,\n",
    "         history_gru.history[\"loss\"], \"r.-\", label=\"Training loss (GRU)\")\n",
    "plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "plt.title('Learning curves')\n",
    "plt.legend(fontsize=14, loc='upper right')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualize prices the predicted by all models: ARIMA, LSTM and GRU\n",
    "\n",
    "data = stock_data.filter(['Close'])\n",
    "train = data[:training_data_len]\n",
    "validation = data[training_data_len:]\n",
    "validation['Predictions_ARIMA'] = predictions_arima\n",
    "validation['Predictions_LSTM'] = predictions_lstm\n",
    "validation['Predictions_GRU'] = predictions_gru\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title('Predicted prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing price in USD')\n",
    "plt.plot(train[-200:])  # Plot last 200 entries\n",
    "plt.plot(validation[[\n",
    "    'Close',\n",
    "    'Predictions_ARIMA',\n",
    "    'Predictions_LSTM',\n",
    "    'Predictions_GRU'\n",
    "]])\n",
    "plt.legend([\n",
    "    'Training prices',\n",
    "    f'Actual prices for {ticker}',\n",
    "    'Predicted prices (ARIMA)',\n",
    "    'Predicted prices (LSTM)',\n",
    "    'Predicted prices (GRU)'\n",
    "],\n",
    "    loc='upper left',\n",
    "    fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "During our experiments performance of each model was based on the RMSE. After comparing the result, models were ranked in the following order from the most to the least accurate:\n",
    "1. `GRU` model constantly showed the best results with the RMSE of `1.00`.\n",
    "2. `LSTM` had RMSE around `3.5`.\n",
    "3. Finally, `ARIMA` architecture proved to be the least accurate with RMSE around of `12`.\n",
    "\n",
    "Admittedly, each model could be further improved with fine-tuning. However, results of the practical projects confirms that **generally** with the large quantity of data deep learning-based algorithms, such as LSTM or GRU, outperform traditional algorithms, such as the ARIMA model. As a result of the literature review, it is noted that the ARIMA model produced better results with a smaller quantity of data in previous academic studies. Reference: [Elsaraiti, M.; Merabet, A. A Comparative Analysis of the ARIMA and LSTM Predictive Models and their Effectiveness for Predicting Wind Speed. Energies 2021, 14, 6782.](https://doi.org/10.3390/en14206782).\n",
    "\n",
    "GRU model had the nice added bonus that it was the fastest to train (even without GPU). It could be explained by the fact that GRUs have simpler internal structure. I.e. LSTMs have 3 gates, GRUs have only 2 gates.\n",
    "\n",
    "It is worth mentioning that it is easier to utilize GPU-acceleration when training model based on RNNs (LSTM and GRU). For example, Google Colab environment by default trained RNN based models using GPU-acceleration. On the other hand, GPU-accelerated ARIMA implementations are also available (for instance: [RAPIDS cuML](https://github.com/rapidsai/cuml)) but are more difficult to implement in practice.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nAIjqDuVFnfD",
    "1g3URNZWFqlj",
    "imi0dVRoUvvF",
    "AVJBBfeEv9fb",
    "kIv0qmZW5GTs",
    "v5Grl-Qf5GUg"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb49f1f2d2fe7b3e28bf680fd8070852ae61e846bb3a58a539a2279b4b315965"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "042fba73a07248b1a969f777d8f91d90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0abc5773aeda4b9386308f2a0907a388": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fe9996d7f8d456e83bcbbd49c658d16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ca61547da6e40a783b00381363fcc1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a4d6d1ef76145c1a29c94e7f8501498": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fe9996d7f8d456e83bcbbd49c658d16",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b91635942c2d4633aa82ca25189d7828",
      "value": 1000
     }
    },
    "87b44aa2691042fe827a440bb2bcaf4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9da1826883194b35b8e22e54ba76591c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ca61547da6e40a783b00381363fcc1d",
      "placeholder": "​",
      "style": "IPY_MODEL_0abc5773aeda4b9386308f2a0907a388",
      "value": "100%"
     }
    },
    "a115aa0b3d114452a92f4604a49327ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9da1826883194b35b8e22e54ba76591c",
       "IPY_MODEL_7a4d6d1ef76145c1a29c94e7f8501498",
       "IPY_MODEL_f2cf992ffac64e55a0bd54716f639170"
      ],
      "layout": "IPY_MODEL_87b44aa2691042fe827a440bb2bcaf4a"
     }
    },
    "a960fbbfdd8b4ecb97e5440f1f6b2917": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b91635942c2d4633aa82ca25189d7828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2cf992ffac64e55a0bd54716f639170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_042fba73a07248b1a969f777d8f91d90",
      "placeholder": "​",
      "style": "IPY_MODEL_a960fbbfdd8b4ecb97e5440f1f6b2917",
      "value": " 1000/1000 [01:44&lt;00:00,  9.76it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
